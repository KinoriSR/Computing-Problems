import sys
import re
import pandas as pd
import numpy as np
from sklearn.ensemble import GradientBoostingRegressor

Input = []
for line in sys.stdin:
    if 'Missing_' in line:
        line=re.sub('Missing_[0-9]+','None', line)
    Input.append(line.strip().split('\t'))
n=int(Input[0][0])
data=pd.DataFrame(Input[2:], columns=Input[1])
# print(data.loc[1:10])

# put in NaN
data.replace('None', np.NaN, inplace=True)
# print(data.loc[1:10])

# data=data.convert_dtypes()
month = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6, 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12}
data['yyyy']=pd.to_numeric(data['yyyy'])
data['month'].replace(month, inplace=True) # can also try one hot encoding
data['tmax']=pd.to_numeric(data['tmax'])
data['tmin']=pd.to_numeric(data['tmin'])

# prep train data
processed_data=data.dropna()

# train model max
max_model=GradientBoostingRegressor().fit(processed_data[['yyyy','month']], processed_data['tmax'])
# train model min
min_model=GradientBoostingRegressor().fit(processed_data[['yyyy','month']], processed_data['tmin'])

for i in range(n):
    row=data.loc[i]
    if pd.isna(row['tmax']):
        # print(pd.DataFrame({'yyyy':pd.Series(row['yyyy']), 'month':pd.Series(row['month'])}).shape) # works
        # print(data.loc[i:i,'yyyy':'month'].shape) # also works
        print(max_model.predict(data.loc[i:i,'yyyy':'month'])[0])
    if pd.isna(row['tmin']):
        print(min_model.predict(data.loc[i:i,'yyyy':'month'])[0])

Expected:
8.6
15.8
18.9
0.0
7.0
2.2
6.0
1.8
12.5
4.8
1.3
9.0
14.9
16.7
3.8
7.2
10.1
7.3
8.5
12.5
3.5
17.5
11.2
6.6
0.3
18.4
15.8
12.5
-0.9
18.7
4.9
9.4
8.0
18.8
19.0
14.1
4.4
-0.4
3.0
9.4
16.3
1.7
2.2
3.3
3.4
17.9
0.1
7.3
16.2
10.9
15.3
12.9
8.6
5.5
0.9
2.3
15.9
13.1
12.3
9.6
14.9
2.5
7.4
8.9
7.4
11.1
11.3
2.6
10.0
16.9
6.4
4.0
6.1
10.2
7.1
8.3
21.4
3.7
-0.2
9.4
11.6
2.7
9.8
20.3
3.0
11.5
6.8
3.2
-0.8
7.4
4.8
10.9
12.4
-3.4
5.7
5.8
2.4
-2.5
3.8
10.9

Mine:
10.67987686338232
14.655207385072444
18.296820819070458
1.965920064920829
5.522575035994961
0.7024317563628962
5.706501508444072
1.190144258246836
10.97098881085097
5.984814285791083
1.5128758441563042
7.352619266343729
14.591065383211772
17.313503053338728
2.388618525829857
6.535634306186498
10.979618598138284
6.1254411288083395
9.138651734273452
12.43979396730907
2.6764117776523815
18.555829367579882
11.78238374254923
6.268350783758488
1.7841605801557952
18.555829367579882
15.817210366828075
11.78238374254923
0.2183672292027227
17.04143494517453
1.8553496484816685
10.918987512006943
9.234931243354902
18.924108425541267
18.446043936039402
16.185489424789463
5.243355322063218
0.7622737171040713
2.924033101298617
10.515030487741447
15.877460635673549
2.717041149245615
1.6617957973882236
2.2000492699917356
3.606214727286882
19.627870124064067
1.4108327677212327
6.028290048998549
18.029419445577997
11.752459914980204
16.693345979272483
10.951295266517127
8.86391878368153
6.874843168879941
1.4101687247896577
3.220877122880206
15.905060337413426
14.617887692363217
11.270433546416903
8.532690078937168
17.504911008428945
6.557854345179828
6.486765028487991
8.028695891940375
7.720606914207865
11.422566653207802
11.351657522526358
2.78907990578522
10.374435235550536
18.213271278213337
6.276705938309328
3.042370803995035
6.79261684609815
9.219714939949284
6.248681614381873
8.858306084903544
19.36882421817887
3.542445959159771
2.7608938569516672
10.997057818049026
11.640241213779495
3.518946171662181
9.472891699863565
18.91391351974507
1.7255815309583187
11.289928974172968
6.679177633229726
4.0798538713585835
-0.6501445725009839
7.263521350153263
6.44669886367124
11.584843427488444
12.672408012497668
-0.29028971200744674
6.441636771422202
8.90140282266326
3.1927671516757115
2.7183914359490413
2.5448970853403288
11.803479114188724